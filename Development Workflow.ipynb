{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gist here is that non-image problems can become image problems if we encode features as pixel channels, and then train an image classifier/regressor on the generated image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Getting Data (handled by user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tabular data; user should ensure that all data is clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we have some tabular data (say, in a dataframe). Since this will be automated, we will brute-force create a large number of features that hopefully describe the data better than the raw input. We will do this by using the open source python module ```featuretools```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Clustering Features and Assigning to Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Ensure that the number of features, $m$, is divisible by the quantity $3\\cdot4^n$ for some $n$. If not, go back to the __Feature Engineering__ step and create more features or remove features in order to satisfy this condition.\n",
    "2. Let the data table with the features (in our case, ```pandas.DataFrame```) be called ```features```. Then we convert this to a numpy array, called $X$. The columns represent values for each feature, and each row represents one entry/example. Now note that in traditional cluster analysis, examples are clustered together by minimizing a distance metric, which is computed by finding the examples' different feature lengths. However, in this case, we actually want to group features by example values. Put another way, we want to group the synthetic features that are closest together in value, and the way we determine their similarity is by seeing how similar the values are for their various examples. Hence, in order to cluster the features, we will perform cluster analysis on $X^T$. When there is a large number of examples in $X$, then $X^T$ will have a large number of columns, implying that cluster analysis on $X^T$ will fall victim to the curse of dimensionality. Dimensionality reduction techniques may become helpful to reduce the number of columns (i.e. decrease the number of examples to be used). One option is to perform PCA and get only the most distinct columns of $X^T$.\n",
    "3. Run the function ```fn()``` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BalancedClusters import BalancedClusters\n",
    "def fn(x_transpose, location_descriptor):\n",
    "    if len(num_examples(x_transpose) == 3):\n",
    "        # populate the three channels at the specified location in the picture\n",
    "        pixel_location = get_pixel_to_populate(location_descriptor)\n",
    "        populate_pixel(pixel_location)\n",
    "    else:\n",
    "        # get balanced clusters for each quadrant\n",
    "        clusters = make_clusters(data=x_transpose, num_clusters=4) # must be a dictionary from name to 2D numpy array\n",
    "        max_iterations = 1000\n",
    "        cluster_balancer = BalancedClusters(clusters,'optimal')\n",
    "        balanced_clusters = cluster_balancer.balance_clusters(max_iterations=max_iterations, verbose='none')\n",
    "        \n",
    "        # get quadrant locations\n",
    "        quadrant_locations = get_location_descriptors(location_descriptor)\n",
    "        \n",
    "        assert len(quadrant_locations) == len(balanced_clusters.keys()) == 4\n",
    "        # call fn recursively on each quadrant\n",
    "        for i, key in enumerate(balanced_clusters.keys()): \n",
    "            fn(balanced_clusters[key], quadrant_locations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea for feature selection/dimensionality reduction (in general):\n",
    "    - Transpose of regular feature array ($X$ -> $X^T$)\n",
    "    - K clusters\n",
    "    - See what examples of $X^T$ are in what groups (i.e. what columns of $X$ are most similar)\n",
    "    - Perform some sort of averaging for each group\n",
    "    - Proceed with only K features    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Wrapping into an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Make Predictions! (handled by user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
